{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fine-tuning BioMedLM (2.7B) with MedQuAD Dataset using LoRA\n",
    "\n",
    "This notebook implements efficient fine-tuning of the BioMedLM model using Low-Rank Adaptation (LoRA) on the MedQuAD dataset containing ~47,000 medical question-answer pairs.\n",
    "\n",
    "## Requirements\n",
    "- GPU with at least 16GB VRAM (e.g., NVIDIA RTX 3090)\n",
    "- CUDA-compatible PyTorch installation\n",
    "- Hugging Face transformers library with PEFT support\n",
    "\n",
    "## Overview\n",
    "1. Data preprocessing and tokenization\n",
    "2. Model setup with LoRA configuration\n",
    "3. Training with gradient accumulation for memory efficiency\n",
    "4. Evaluation on validation split\n",
    "5. Model saving in SafeTensors format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell to install all necessary dependencies\n",
    "\n",
    "!pip install transformers==4.36.0\n",
    "!pip install peft==0.7.1\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install datasets==2.16.0\n",
    "!pip install safetensors==0.4.1\n",
    "!pip install rouge-score==0.1.2\n",
    "!pip install nltk==3.8.1\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data for evaluation\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"stanford-crfm/BioMedLM\"  # 2.7B parameter model\n",
    "    MAX_LENGTH = 512  # Maximum sequence length\n",
    "    \n",
    "    # LoRA configuration\n",
    "    LORA_R = 16  # LoRA rank\n",
    "    LORA_ALPHA = 32  # LoRA alpha\n",
    "    LORA_DROPOUT = 0.1  # LoRA dropout\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 4  # Batch size per device\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 100\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Data configuration\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    SEED = 42\n",
    "    \n",
    "    # Output paths\n",
    "    OUTPUT_DIR = \"./biomedlm-lora-medquad\"\n",
    "    CHECKPOINT_DIR = \"./checkpoints\"\n",
    "    \n",
    "    # Quantization configuration for memory efficiency\n",
    "    LOAD_IN_8BIT = False  # Set to True if you have memory constraints\n",
    "    LOAD_IN_4BIT = True   # More aggressive quantization\n",
    "    \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Load and Preprocess the MedQuAD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MedQuAD dataset\n",
    "print(\"Loading MedQuAD dataset...\")\n",
    "df = pd.read_csv('medquad.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few examples:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing questions or answers\n",
    "df = df.dropna(subset=['question', 'answer'])\n",
    "print(f\"\\nDataset shape after removing missing values: {df.shape}\")\n",
    "\n",
    "# Create instruction-following format for fine-tuning\n",
    "def create_prompt(question, answer=None):\n",
    "    \"\"\"Create a prompt in instruction-following format\"\"\"\n",
    "    prompt = f\"\"\"Below is a medical question. Provide a detailed, evidence-based answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    if answer:\n",
    "        prompt += f\" {answer}\"\n",
    "    return prompt\n",
    "\n",
    "# Apply formatting\n",
    "df['text'] = df.apply(lambda row: create_prompt(row['question'], row['answer']), axis=1)\n",
    "\n",
    "# Display sample formatted text\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(df['text'].iloc[0][:500] + \"...\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=config.VALIDATION_SPLIT, \n",
    "    random_state=config.SEED,\n",
    "    stratify=df['source'] if 'source' in df.columns else None\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = HFDataset.from_pandas(train_df[['text']])\n",
    "val_dataset = HFDataset.from_pandas(val_df[['text']])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Load and Configure the Model with LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure quantization for memory efficiency\n",
    "if config.LOAD_IN_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "elif config.LOAD_IN_8BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading BioMedLM model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "if config.LOAD_IN_4BIT or config.LOAD_IN_8BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "print(\"Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Tokenize the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text examples\"\"\"\n",
    "    # Tokenize with truncation and padding\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Set labels (same as input_ids for causal LM)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train dataset\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized train samples: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized validation samples: {len(tokenized_val)}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8  # For tensor core efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Define Training Arguments and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=config.WARMUP_STEPS,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    fp16=True,  # Mixed precision training\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    group_by_length=True,  # Group sequences of similar length\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=True,\n",
    "    seed=config.SEED,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Print training configuration summary\n",
    "total_steps = len(tokenized_train) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS) * config.NUM_EPOCHS\n",
    "print(f\"\\nTraining Configuration Summary:\")\n",
    "print(f\"- Total training samples: {len(tokenized_train)}\")\n",
    "print(f\"- Total validation samples: {len(tokenized_val)}\")\n",
    "print(f\"- Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"- Total training steps: {total_steps}\")\n",
    "print(f\"- Warmup steps: {config.WARMUP_STEPS}\")\n",
    "print(f\"- Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"- LoRA rank: {config.LORA_R}\")\n",
    "print(f\"- LoRA alpha: {config.LORA_ALPHA}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save training metrics\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "print(f\"\\nModel saved to: {config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, tokenizer, eval_dataset, num_samples=100):\n",
    "    \"\"\"Evaluate model on a subset of the validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results = {\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': [],\n",
    "        'exact_match': [],\n",
    "        'predictions': []\n",
    "    }\n",
    "    \n",
    "    # Sample evaluation examples\n",
    "    eval_samples = eval_dataset.select(range(min(num_samples, len(eval_dataset))))\n",
    "    \n",
    "    print(f\"Evaluating on {len(eval_samples)} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(tqdm(eval_samples, desc=\"Evaluating\")):\n",
    "            # Extract question and reference answer\n",
    "            text = sample['text']\n",
    "            parts = text.split(\"Answer:\")\n",
    "            if len(parts) == 2:\n",
    "                question_part = parts[0] + \"Answer:\"\n",
    "                reference_answer = parts[1].strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                question_part,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=config.MAX_LENGTH,\n",
    "                truncation=True\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate prediction\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode prediction\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Extract only the generated answer part\n",
    "            if \"Answer:\" in prediction:\n",
    "                prediction = prediction.split(\"Answer:\")[-1].strip()\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            scores = scorer.score(reference_answer, prediction)\n",
    "            results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "            \n",
    "            # Check exact match (normalized)\n",
    "            exact_match = reference_answer.lower().strip() == prediction.lower().strip()\n",
    "            results['exact_match'].append(exact_match)\n",
    "            \n",
    "            # Store prediction for analysis\n",
    "            results['predictions'].append({\n",
    "                'question': question_part.split(\"Question:\")[-1].split(\"Answer:\")[0].strip(),\n",
    "                'reference': reference_answer[:200] + \"...\" if len(reference_answer) > 200 else reference_answer,\n",
    "                'prediction': prediction[:200] + \"...\" if len(prediction) > 200 else prediction\n",
    "            })\n",
    "            \n",
    "            # Print sample predictions\n",
    "            if idx < 3:\n",
    "                print(f\"\\nExample {idx + 1}:\")\n",
    "                print(f\"Question: {results['predictions'][-1]['question']}\")\n",
    "                print(f\"Reference: {results['predictions'][-1]['reference']}\")\n",
    "                print(f\"Prediction: {results['predictions'][-1]['prediction']}\")\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_results = {\n",
    "        'rouge1': np.mean(results['rouge1']),\n",
    "        'rouge2': np.mean(results['rouge2']),\n",
    "        'rougeL': np.mean(results['rougeL']),\n",
    "        'exact_match': np.mean(results['exact_match'])\n",
    "    }\n",
    "    \n",
    "    return avg_results, results\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nRunning evaluation on validation set...\")\n",
    "avg_scores, detailed_results = evaluate_model(model, tokenizer, val_dataset, num_samples=100)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROUGE-1 F1 Score: {avg_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {avg_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L F1 Score: {avg_scores['rougeL']:.4f}\")\n",
    "print(f\"Exact Match Accuracy: {avg_scores['exact_match']:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "with open(f\"{config.OUTPUT_DIR}/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        'average_scores': avg_scores,\n",
    "        'sample_predictions': detailed_results['predictions'][:10]\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {config.OUTPUT_DIR}/evaluation_results.json\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Save Model in SafeTensors Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in SafeTensors format\n",
    "from safetensors.torch import save_file\n",
    "import os\n",
    "\n",
    "print(\"Saving model in SafeTensors format...\")\n",
    "\n",
    "# Create directory for SafeTensors\n",
    "safetensors_dir = f\"{config.OUTPUT_DIR}/safetensors\"\n",
    "os.makedirs(safetensors_dir, exist_ok=True)\n",
    "\n",
    "# Get the LoRA weights\n",
    "lora_state_dict = model.get_peft_state_dict()\n",
    "\n",
    "# Save LoRA weights in SafeTensors format\n",
    "save_file(lora_state_dict, f\"{safetensors_dir}/adapter_model.safetensors\")\n",
    "\n",
    "# Save the configuration\n",
    "model.peft_config['default'].save_pretrained(safetensors_dir)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(safetensors_dir)\n",
    "\n",
    "print(f\"Model saved in SafeTensors format at: {safetensors_dir}\")\n",
    "\n",
    "# Also save the merged model (optional, requires more disk space)\n",
    "print(\"\\nMerging LoRA weights with base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_dir = f\"{config.OUTPUT_DIR}/merged_model\"\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "print(f\"Merged model saved at: {merged_dir}\")\n",
    "\n",
    "# Calculate model sizes\n",
    "import os\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024**3)  # Convert to GB\n",
    "\n",
    "lora_size = get_dir_size(safetensors_dir)\n",
    "print(f\"\\nLoRA adapter size: {lora_size:.2f} GB\")\n",
    "if os.path.exists(merged_dir):\n",
    "    merged_size = get_dir_size(merged_dir)\n",
    "    print(f\"Merged model size: {merged_size:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: Test the Fine-tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers for test questions\n",
    "def generate_medical_answer(question, model, tokenizer, max_length=512):\n",
    "    \"\"\"Generate an answer for a medical question\"\"\"\n",
    "    # Format the question\n",
    "    prompt = create_prompt(question)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer\n",
    "    if \"Answer:\" in response:\n",
    "        answer = response.split(\"Answer:\")[-1].strip()\n",
    "    else:\n",
    "        answer = response\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with sample medical questions\n",
    "test_questions = [\n",
    "    \"What are the main symptoms of diabetes?\",\n",
    "    \"How is hypertension diagnosed?\",\n",
    "    \"What are the treatment options for migraine headaches?\",\n",
    "    \"What causes glaucoma and how can it be prevented?\",\n",
    "    \"What are the side effects of chemotherapy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model with sample questions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\nQuestion {i+1}: {question}\")\n",
    "    answer = generate_medical_answer(question, model, tokenizer)\n",
    "    print(f\"Answer: {answer[:500]}...\" if len(answer) > 500 else f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Fine-tuning Process Summary\n",
    "\n",
    "1. **Model**: BioMedLM (2.7B parameters) - a biomedically pre-trained language model\n",
    "2. **Dataset**: MedQuAD with ~47,000 medical question-answer pairs\n",
    "3. **Method**: Low-Rank Adaptation (LoRA) for efficient fine-tuning\n",
    "4. **Hardware Requirements**: GPU with 16GB+ VRAM (e.g., RTX 3090)\n",
    "\n",
    "### Key Configuration Details\n",
    "\n",
    "- **LoRA Parameters**:\n",
    "  - Rank (r): 16\n",
    "  - Alpha: 32\n",
    "  - Target modules: All attention and MLP projection layers\n",
    "  - Dropout: 0.1\n",
    "\n",
    "- **Training Parameters**:\n",
    "  - Effective batch size: 16 (4 Ã— 4 gradient accumulation)\n",
    "  - Learning rate: 2e-5\n",
    "  - Epochs: 3\n",
    "  - Max sequence length: 512 tokens\n",
    "  - Mixed precision training (FP16)\n",
    "  - 4-bit quantization for memory efficiency\n",
    "\n",
    "### Expected Performance Metrics\n",
    "\n",
    "Based on similar fine-tuning tasks, you can expect:\n",
    "- ROUGE-1: 0.35-0.45\n",
    "- ROUGE-2: 0.15-0.25\n",
    "- ROUGE-L: 0.30-0.40\n",
    "- Exact Match: 0.05-0.15\n",
    "\n",
    "### Potential Challenges and Solutions\n",
    "\n",
    "1. **Memory Constraints**:\n",
    "   - Solution: Use 4-bit quantization, gradient checkpointing, and gradient accumulation\n",
    "   - Alternative: Reduce batch size or max sequence length\n",
    "\n",
    "2. **Training Instability**:\n",
    "   - Solution: Lower learning rate, increase warmup steps\n",
    "   - Monitor loss curves for spikes\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - Solution: Increase dropout, reduce epochs, or use early stopping\n",
    "   - Monitor validation loss closely\n",
    "\n",
    "4. **Poor Generation Quality**:\n",
    "   - Solution: Adjust generation parameters (temperature, top_p)\n",
    "   - Fine-tune on higher quality subset of data\n",
    "\n",
    "### Recommendations for Further Optimization\n",
    "\n",
    "1. **Data Quality Enhancement**:\n",
    "   - Filter out low-quality Q&A pairs\n",
    "   - Add data augmentation (paraphrasing questions)\n",
    "   - Include more diverse medical sources\n",
    "\n",
    "2. **Advanced Training Techniques**:\n",
    "   - Implement curriculum learning (easy to hard questions)\n",
    "   - Use different LoRA ranks for different layer types\n",
    "   - Experiment with QLoRA for even better memory efficiency\n",
    "\n",
    "3. **Evaluation Improvements**:\n",
    "   - Add domain-specific metrics (medical accuracy)\n",
    "   - Human evaluation by medical professionals\n",
    "   - Test on out-of-distribution medical questions\n",
    "\n",
    "4. **Deployment Optimization**:\n",
    "   - Quantize the final model to INT8 for inference\n",
    "   - Use Flash Attention for faster generation\n",
    "   - Implement caching for common questions\n",
    "\n",
    "5. **Safety Considerations**:\n",
    "   - Add disclaimer about not replacing medical professionals\n",
    "   - Implement confidence scoring for answers\n",
    "   - Filter potentially harmful medical advice\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "To use the fine-tuned model after training:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"stanford-crfm/BioMedLM\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./biomedlm-lora-medquad/safetensors\")\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, \"./biomedlm-lora-medquad/safetensors\")\n",
    "\n",
    "# Generate answer\n",
    "question = \"What are the symptoms of COVID-19?\"\n",
    "answer = generate_medical_answer(question, model, tokenizer)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Fine-tuning notebook completed successfully!\")\n",
    "print(f\"Models saved at:\")\n",
    "print(f\"  - LoRA adapter: {config.OUTPUT_DIR}/safetensors\")\n",
    "print(f\"  - Merged model: {config.OUTPUT_DIR}/merged_model\")\n",
    "print(f\"  - Checkpoints: {config.CHECKPOINT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
